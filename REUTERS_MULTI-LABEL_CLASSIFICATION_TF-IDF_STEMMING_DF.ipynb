{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c342cd",
   "metadata": {},
   "source": [
    "# import needed libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41c5051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords,reuters\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "stop_words = stopwords.words(\"english\")\n",
    "from sklearn.metrics import accuracy_score , classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, hamming_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf14bc",
   "metadata": {},
   "source": [
    "# Represent train and test docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba20fd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs in the train set: 7769\n",
      "Number of docs in the test set: 3019\n"
     ]
    }
   ],
   "source": [
    "#extracting train and test documents\n",
    "#train_documents\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"),reuters.fileids()));\n",
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "print('Number of docs in the train set: ' + str(len(train_docs)))\n",
    "#test_documents\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"),reuters.fileids()));\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "print('Number of docs in the test set: ' + str(len(test_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810317d7",
   "metadata": {},
   "source": [
    "# Cleaning Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7797d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['\\r', '\\n', '\\t','lt']\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower() #Make text lowercase\n",
    "    text = re.sub('\\[.*?\\]', '',str(text))\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', str(text)) #remove punctuation\n",
    "    text = re.sub('\\w*\\d\\w*', '', str(text)) #remove words containing numbers\n",
    "    text = re.sub('[‘’“”…]', '', str(text))\n",
    "    text = re.sub(r'dlrs', 'dollar', text)  # replace dlrs abreviation \n",
    "    text = re.sub(r'pct', 'percent', text)  # replace pct abreviation  \n",
    "    for code in codes:\n",
    "        text = re.sub(code, ' ', text)  # get rid of escape codes\n",
    "    text = re.sub('\\s+', ' ', text) # replace multiple spacess with one space   \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1cdd42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning training and testing documents\n",
    "cleaned_train_documents = []\n",
    "for i in range(0,len(train_docs)):    \n",
    "    cleaned_train_documents.append(clean_text(str(train_docs[i])))\n",
    "cleaned_test_documents = []\n",
    "for i in range(0,len(test_docs)):    \n",
    "    cleaned_test_documents.append(clean_text(str(test_docs[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f87c9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning training and testing documents\n",
    "cleaned_train_documents = []\n",
    "for i in range(0,len(train_docs)):    \n",
    "    cleaned_train_documents.append(clean_text(str(train_docs[i])))\n",
    "cleaned_test_documents = []\n",
    "for i in range(0,len(test_docs)):    \n",
    "    cleaned_test_documents.append(clean_text(str(test_docs[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0493f1",
   "metadata": {},
   "source": [
    "# Tozenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db60055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n",
    "def toknize(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text) #split document into individual words(tokens)\n",
    "    tokens= [word for word in tokens if word not in stop_words] #filter out stop words if requested\n",
    "    tokens = [word for word in tokens if word.isalpha()] #remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if len(word) > 2] #filter out tokens that are one or two characters long\n",
    "    tokens = [word for word in tokens if len(word) < 21] # filter out tokens that are more than twenty characters long\n",
    "    stemmas= [stemmer.stem(word) for word in tokens]\n",
    "    # recreate the document string from parsed words\n",
    "    text = ''\n",
    "    for stem in stemmas:\n",
    "        text = text + ' ' + stem\n",
    "    return tokens,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f052ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [] # list of document strings for sklearn TF-IDF\n",
    "train_tokens = []  # list of token lists\n",
    "for doc in cleaned_train_documents:\n",
    "    text_string = doc\n",
    "    # parse words one at a time in document string\n",
    "    tokens,text_string = toknize(text_string)\n",
    "    train_tokens.append(tokens)\n",
    "    train_text.append(text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5540c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = []  # list of token lists\n",
    "test_text = [] # list of document strings for sklearn TF-IDF\n",
    "for doc in cleaned_test_documents:\n",
    "    text_string = doc\n",
    "    # parse words one at a time in document string\n",
    "    tokens,text_string = toknize(text_string)\n",
    "    test_tokens.append(text_string)\n",
    "    test_text.append(text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30c212",
   "metadata": {},
   "source": [
    "# Applying (TF-IDF vectorizer)and(document frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f401fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn and transform train documents\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize,max_df=0.95, min_df=3,max_features=1000)\n",
    "vectorised_train_documents = vectorizer.fit_transform(train_text)\n",
    "# transform test documents\n",
    "vectorised_test_documents = vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdadbec",
   "metadata": {},
   "source": [
    "# Transform Multilabel labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18da2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode multi-label per instance \n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([reuters.categories(doc_id)\n",
    "for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([reuters.categories(doc_id)\n",
    "for doc_id in test_docs_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67422b39",
   "metadata": {},
   "source": [
    "# Train and Evaluate Classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6c7a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelsPerformance = {}\n",
    "\n",
    "def metricsReport(modelName, test_labels, predictions):\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "\n",
    "    macro_precision = precision_score(test_labels, predictions, average='macro')\n",
    "    macro_recall = recall_score(test_labels, predictions, average='macro')\n",
    "    macro_f1 = f1_score(test_labels, predictions, average='macro')\n",
    "\n",
    "    micro_precision = precision_score(test_labels, predictions, average='micro')\n",
    "    micro_recall = recall_score(test_labels, predictions, average='micro')\n",
    "    micro_f1 = f1_score(test_labels, predictions, average='micro')\n",
    "    hamLoss = hamming_loss(test_labels, predictions)\n",
    "    print(\"------\" + modelName + \" Model Metrics-----\")\n",
    "    print(\"Accuracy: {:.4f}\\nHamming Loss: {:.4f}\\nPrecision:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\\nRecall:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\\nF1-measure:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\"\\\n",
    "          .format(accuracy, hamLoss, macro_precision, micro_precision, macro_recall, micro_recall, macro_f1, micro_f1))\n",
    "    ModelsPerformance[modelName] = micro_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6425389",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aeec175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda_new\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Random Forest Model Metrics-----\n",
      "Accuracy: 0.7082\n",
      "Hamming Loss: 0.0051\n",
      "Precision:\n",
      "  - Macro: 0.4024\n",
      "  - Micro: 0.9576\n",
      "Recall:\n",
      "  - Macro: 0.1383\n",
      "  - Micro: 0.6568\n",
      "F1-measure:\n",
      "  - Macro: 0.1855\n",
      "  - Micro: 0.7792\n"
     ]
    }
   ],
   "source": [
    "rfClassifier = RandomForestClassifier(n_jobs=-1)\n",
    "rfClassifier.fit(vectorised_train_documents, train_labels)\n",
    "rfPreds = rfClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Random Forest\", test_labels, rfPreds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c0a53",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09bb4673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda_new\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Decision Tree Model Metrics-----\n",
      "Accuracy: 0.7271\n",
      "Hamming Loss: 0.0066\n",
      "Precision:\n",
      "  - Macro: 0.3557\n",
      "  - Micro: 0.7752\n",
      "Recall:\n",
      "  - Macro: 0.3119\n",
      "  - Micro: 0.7369\n",
      "F1-measure:\n",
      "  - Macro: 0.3226\n",
      "  - Micro: 0.7556\n"
     ]
    }
   ],
   "source": [
    "dtClassifier = DecisionTreeClassifier()\n",
    "dtClassifier.fit(vectorised_train_documents, train_labels)\n",
    "dtPreds = dtClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Decision Tree\", test_labels, dtPreds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e2fd0",
   "metadata": {},
   "source": [
    "# Support Vector Machine Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "216af5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda_new\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------SVC Sq. Hinge Loss Model Metrics-----\n",
      "Accuracy: 0.8003\n",
      "Hamming Loss: 0.0036\n",
      "Precision:\n",
      "  - Macro: 0.5803\n",
      "  - Micro: 0.9407\n",
      "Recall:\n",
      "  - Macro: 0.3353\n",
      "  - Micro: 0.7917\n",
      "F1-measure:\n",
      "  - Macro: 0.3974\n",
      "  - Micro: 0.8598\n"
     ]
    }
   ],
   "source": [
    "svmClassifier = OneVsRestClassifier(LinearSVC(), n_jobs=-1)\n",
    "svmClassifier.fit(vectorised_train_documents, train_labels)\n",
    "\n",
    "svmPreds = svmClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"SVC Sq. Hinge Loss\", test_labels, svmPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca823de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Power Set SVC Model Metrics-----\n",
      "Accuracy: 0.8370\n",
      "Hamming Loss: 0.0039\n",
      "Precision:\n",
      "  - Macro: 0.5990\n",
      "  - Micro: 0.8929\n",
      "Recall:\n",
      "  - Macro: 0.4080\n",
      "  - Micro: 0.8130\n",
      "F1-measure:\n",
      "  - Macro: 0.4608\n",
      "  - Micro: 0.8511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda_new\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "powerSetSVC = LabelPowerset(LinearSVC())\n",
    "powerSetSVC.fit(vectorised_train_documents, train_labels)\n",
    "\n",
    "powerSetSVCPreds = powerSetSVC.predict(vectorised_test_documents)\n",
    "metricsReport(\"Power Set SVC\", test_labels, powerSetSVCPreds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63806998",
   "metadata": {},
   "source": [
    "# Naive Base Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da7fe5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Multinomial NB Model Metrics-----\n",
      "Accuracy: 0.6870\n",
      "Hamming Loss: 0.0055\n",
      "Precision:\n",
      "  - Macro: 0.2388\n",
      "  - Micro: 0.8973\n",
      "Recall:\n",
      "  - Macro: 0.1260\n",
      "  - Micro: 0.6765\n",
      "F1-measure:\n",
      "  - Macro: 0.1446\n",
      "  - Micro: 0.7714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda_new\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "nbClassifier = OneVsRestClassifier(MultinomialNB())\n",
    "nbClassifier.fit(vectorised_train_documents, train_labels)\n",
    "\n",
    "nbPreds = nbClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Multinomial NB\", test_labels, nbPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15c55cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model Name           | Micro-F1 Score\n",
      "-------------------------------------------\n",
      "  Random Forest        | 0.7791508238276298\n",
      "-------------------------------------------\n",
      "  Decision Tree        | 0.7555798986717789\n",
      "-------------------------------------------\n",
      "  SVC Sq. Hinge Loss   | 0.8597534445250181\n",
      "-------------------------------------------\n",
      "  Power Set SVC        | 0.8511114217810708\n",
      "-------------------------------------------\n",
      "  Multinomial NB       | 0.771432922186691\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"  Model Name \" + \" \"*10 + \"| Micro-F1 Score\")\n",
    "print(\"-------------------------------------------\")\n",
    "for key, value in ModelsPerformance.items():\n",
    "    print(\"  \" + key, \" \"*(20-len(key)) + \"|\", value)\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de109e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
