{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Neccessry libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords,reuters\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "stop_words = stopwords.words(\"english\")\n",
    "from sklearn.metrics import accuracy_score , classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, hamming_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Represent Train And Test Docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs in the train set: 7769\n",
      "Number of docs in the test set: 3019\n"
     ]
    }
   ],
   "source": [
    "#extracting train and test documents\n",
    "#train_documents\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"),reuters.fileids()));\n",
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "print('Number of docs in the train set: ' + str(len(train_docs)))\n",
    "#test_documents\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"),reuters.fileids()));\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "print('Number of docs in the test set: ' + str(len(test_docs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['\\r', '\\n', '\\t','lt']\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower() #Make text lowercase\n",
    "    text = re.sub('\\[.*?\\]', '',str(text))\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', str(text)) #remove punctuation\n",
    "    text = re.sub('\\w*\\d\\w*', '', str(text)) #remove words containing numbers\n",
    "    text = re.sub('[‘’“”…]', '', str(text))\n",
    "    text = re.sub(r'dlrs', 'dollar', text)  # replace dlrs abreviation \n",
    "    text = re.sub(r'pct', 'percent', text)  # replace pct abreviation  \n",
    "    for code in codes:\n",
    "        text = re.sub(code, ' ', text)  # get rid of escape codes\n",
    "    text = re.sub('\\s+', ' ', text) # replace multiple spacess with one space   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning training and testing documents\n",
    "cleaned_train_documents = []\n",
    "for i in range(0,len(train_docs)):    \n",
    "    cleaned_train_documents.append(clean_text(str(train_docs[i])))\n",
    "cleaned_test_documents = []\n",
    "for i in range(0,len(test_docs)):    \n",
    "    cleaned_test_documents.append(clean_text(str(test_docs[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toknize(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text) #split document into individual words(tokens)\n",
    "    tokens= [word for word in tokens if word not in stop_words] #filter out stop words if requested\n",
    "    tokens = [word for word in tokens if word.isalpha()] #remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if len(word) > 2] #filter out tokens that are one or two characters long\n",
    "    tokens = [word for word in tokens if len(word) < 21] # filter out tokens that are more than twenty characters long\n",
    "    # recreate the document string from parsed words\n",
    "    text = ''\n",
    "    for token in tokens:\n",
    "        text = text + ' ' + token\n",
    "    return tokens,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [] # list of document strings for sklearn TF-IDF\n",
    "train_tokens = []  # list of token lists\n",
    "for doc in cleaned_train_documents:\n",
    "    text_string = doc\n",
    "    # parse words one at a time in document string\n",
    "    tokens,text_string = toknize(text_string)\n",
    "    train_tokens.append(tokens)\n",
    "    train_text.append(text_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = []  # list of token lists\n",
    "test_text = [] # list of document strings for sklearn TF-IDF\n",
    "for doc in cleaned_test_documents:\n",
    "    text_string = doc\n",
    "    # parse words one at a time in document string\n",
    "    tokens,text_string = toknize(text_string)\n",
    "    test_tokens.append(text_string)\n",
    "    test_text.append(text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying  (Count Vectorizer)(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer=word_tokenize,max_df=0.95, min_df=3,max_features=1000)\n",
    "# Learn and transform train documents\n",
    "vectorised_train_documents = count_vectorizer.fit_transform(train_text)\n",
    "# transform test documents\n",
    "vectorised_test_documents= count_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform multilabel labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode multi-label per instance \n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([reuters.categories(doc_id)\n",
    "for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([reuters.categories(doc_id)\n",
    "for doc_id in test_docs_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate Classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelsPerformance = {}\n",
    "\n",
    "def metricsReport(modelName, test_labels, predictions):\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "\n",
    "    macro_precision = precision_score(test_labels, predictions, average='macro')\n",
    "    macro_recall = recall_score(test_labels, predictions, average='macro')\n",
    "    macro_f1 = f1_score(test_labels, predictions, average='macro')\n",
    "\n",
    "    micro_precision = precision_score(test_labels, predictions, average='micro')\n",
    "    micro_recall = recall_score(test_labels, predictions, average='micro')\n",
    "    micro_f1 = f1_score(test_labels, predictions, average='micro')\n",
    "    hamLoss = hamming_loss(test_labels, predictions)\n",
    "    print(\"------\" + modelName + \" Model Metrics-----\")\n",
    "    print(\"Accuracy: {:.4f}\\nHamming Loss: {:.4f}\\nPrecision:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\\nRecall:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\\nF1-measure:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\"\\\n",
    "          .format(accuracy, hamLoss, macro_precision, micro_precision, macro_recall, micro_recall, macro_f1, micro_f1))\n",
    "    ModelsPerformance[modelName] = micro_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda_new\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Random Forest Model Metrics-----\n",
      "Accuracy: 0.7122\n",
      "Hamming Loss: 0.0050\n",
      "Precision:\n",
      "  - Macro: 0.4245\n",
      "  - Micro: 0.9562\n",
      "Recall:\n",
      "  - Macro: 0.1496\n",
      "  - Micro: 0.6651\n",
      "F1-measure:\n",
      "  - Macro: 0.1994\n",
      "  - Micro: 0.7845\n"
     ]
    }
   ],
   "source": [
    "rfClassifier = RandomForestClassifier(n_jobs=-1)\n",
    "rfClassifier.fit(vectorised_train_documents, train_labels)\n",
    "rfPreds = rfClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Random Forest\", test_labels, rfPreds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda_new\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Decision Tree Model Metrics-----\n",
      "Accuracy: 0.7304\n",
      "Hamming Loss: 0.0065\n",
      "Precision:\n",
      "  - Macro: 0.3997\n",
      "  - Micro: 0.7778\n",
      "Recall:\n",
      "  - Macro: 0.3194\n",
      "  - Micro: 0.7404\n",
      "F1-measure:\n",
      "  - Macro: 0.3396\n",
      "  - Micro: 0.7586\n"
     ]
    }
   ],
   "source": [
    "dtClassifier = DecisionTreeClassifier()\n",
    "dtClassifier.fit(vectorised_train_documents, train_labels)\n",
    "dtPreds = dtClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Decision Tree\", test_labels, dtPreds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda_new\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------SVC Sq. Hinge Loss Model Metrics-----\n",
      "Accuracy: 0.7406\n",
      "Hamming Loss: 0.0046\n",
      "Precision:\n",
      "  - Macro: 0.5044\n",
      "  - Micro: 0.9000\n",
      "Recall:\n",
      "  - Macro: 0.2924\n",
      "  - Micro: 0.7473\n",
      "F1-measure:\n",
      "  - Macro: 0.3487\n",
      "  - Micro: 0.8166\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler  #if its a dense matrix else use MaxAbsScaler in case of sparse matrix\n",
    "scaler = MaxAbsScaler()\n",
    "X_train = scaler.fit_transform(vectorised_train_documents)\n",
    "X_test = scaler.transform(vectorised_test_documents)\n",
    "svmClassifier = OneVsRestClassifier(LinearSVC(), n_jobs=-1)\n",
    "svmClassifier.fit(X_train, train_labels)\n",
    "\n",
    "svmPreds = svmClassifier.predict(X_test)\n",
    "metricsReport(\"SVC Sq. Hinge Loss\", test_labels, svmPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda_new\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Power Set SVC Model Metrics-----\n",
      "Accuracy: 0.7920\n",
      "Hamming Loss: 0.0055\n",
      "Precision:\n",
      "  - Macro: 0.4618\n",
      "  - Micro: 0.8118\n",
      "Recall:\n",
      "  - Macro: 0.3570\n",
      "  - Micro: 0.7786\n",
      "F1-measure:\n",
      "  - Macro: 0.3818\n",
      "  - Micro: 0.7948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda_new\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "powerSetSVC = LabelPowerset(LinearSVC())\n",
    "powerSetSVC.fit(vectorised_train_documents, train_labels)\n",
    "\n",
    "powerSetSVCPreds = powerSetSVC.predict(vectorised_test_documents)\n",
    "metricsReport(\"Power Set SVC\", test_labels, powerSetSVCPreds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Base Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Multinomial NB Model Metrics-----\n",
      "Accuracy: 0.5449\n",
      "Hamming Loss: 0.0344\n",
      "Precision:\n",
      "  - Macro: 0.1230\n",
      "  - Micro: 0.2788\n",
      "Recall:\n",
      "  - Macro: 0.6886\n",
      "  - Micro: 0.9423\n",
      "F1-measure:\n",
      "  - Macro: 0.1885\n",
      "  - Micro: 0.4302\n"
     ]
    }
   ],
   "source": [
    "nbClassifier = OneVsRestClassifier(MultinomialNB())\n",
    "nbClassifier.fit(vectorised_train_documents, train_labels)\n",
    "\n",
    "nbPreds = nbClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Multinomial NB\", test_labels, nbPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model Name           | Micro-F1 Score\n",
      "-------------------------------------------\n",
      "  Random Forest        | 0.7844990548204159\n",
      "-------------------------------------------\n",
      "  SVC Sq. Hinge Loss   | 0.816576681745221\n",
      "-------------------------------------------\n",
      "  Power Set SVC        | 0.7948193592365371\n",
      "-------------------------------------------\n",
      "  Decision Tree        | 0.7586206896551725\n",
      "-------------------------------------------\n",
      "  Multinomial NB       | 0.43024390243902433\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"  Model Name \" + \" \"*10 + \"| Micro-F1 Score\")\n",
    "print(\"-------------------------------------------\")\n",
    "for key, value in ModelsPerformance.items():\n",
    "    print(\"  \" + key, \" \"*(20-len(key)) + \"|\", value)\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
